AI & Music


Does AI threaten to put us all out of our jobs  or will it simply make us better at doing them? We talked to the developers leading the AI revolution in music.
Artificial intelligence is making an impact almost everywhere. Self-driving cars and virtual assistants are promising to revolutionise everyday life, and music production is also afertile area for research. So what does AI have to offer music producers, and is it aboon or athreat to people who make music for aliving? To find out, we interviewed some leading experts in AI music production. Their responses offer afascinating insight into how current research and developments could change how we produce and listen to music in the future.
Conventional computer software such as adigital audio workstation or plug-ins consists of aset of instructions created by acomputer programmer. These instructions interpret the data and user input we provide when interacting with the software, and the software calculates aset result. For example, when we use acompressor plug-in to process avocal recording, the plug-in responds to our settings and to the dynamics of the material in apredictable way. This can be very useful, but its effectiveness depends entirely on how we set up the compression parameters. Adifferent vocal recording might require adifferent compression ratio, or indeed acompletely different processing chain. Beyond providing presets that we can try out, conventional software cant help us make this decision. Instead, we draw upon our own past experiences and learned skills to decide the settings for the new vocal recording.Potentially, an AI compressor algorithm could analyse aparticular vocalists performance, compare it with alibrary of other performances and generate appropriate compression settings.Artificial intelligence software could emulate these cognitive skills. Rather than simply offering adial for the user to set compression ratio, an AI-based compressor could figure out for itself what the best setting might be. Through the analysis of large numbers of existing vocal recordings, the software would build up amathematical model of what good vocal compression sounds like, and apply this model to new recordings by comparing them with the patterns it has learned, this setting the compressor on the basis of its own learned experience, rather than ours.
In our scenario, the user simply wants the vocal to be more consistent in level. AI software could apply its model learned from earlier vocal recordings to automatically adjust the compression ratio, threshold and other settings, without requiring the user to apply his or her own past experience, learned skills or decision-making.
This is amodest theoretical illustration of how computing concepts such as artificial intelligence, machine learning and big data could offer new possibilities. These computing concepts are equally applicable to many other music production scenarios, and could profoundly change how we produce music. In fact, AI is already having asignificant impact in areas such as mastering and composition.
Tamer Rashad is founder and CEO of Humtap.Tamer Rashad is CEO of Humtap, who have invested 100,000 hours of R&D with musicologists, artists, record companies and producers to develop AI algorithms for their eponymous iOS music production app. As Tamer explains, Humtap listens to your voice and turns your hums into songs in the style of your favourite artist, in seconds and on the fly on your mobile phone. The app records your hummed melody and chosen rhythm and applies AI algorithms to compose, arrange, perform and produce an instrumental track  all you need to do is hum your tune and select adrum track on your iPhones screen. You can even choose amusical genre, such as Depeche Mode or Metallica.
Many high-profile artists are collaborating with Humtaps software developers, new genres are being continuously added and the algorithms are evolving. AI-created vocalists are some way off, although Tamer hasnt ruled this out in the longer term, and an Android version is being developed. Tamer predicts afuture whereby anyone without musical training, studio equipment, financial resources or access to music producers can produce an album  potentially within seconds, rather than ayear. AI will do the production and mixing for you, all for the cost of asmartphone.

Several companies already offer online automated AI mastering services. LANDRs AI-powered mastering engine listens to your unmastered song, identifies the genre and applies relevant mastering equalisation, multiband compression and other processing, all without human intervention. The processing is adaptive, responding to the needs of the song by continuously tweaking the EQ, compression and other processing tools throughout the track. Each time LANDR masters atrack and listens to new music, the better it becomes, thanks to self-learning algorithms.
Pascal Pilon from LANDR claims: Were mastering ahuge variety of music at avery professional level. LANDR has reached apoint where its difficult for even professionals to know if asong has been mastered by areal person or by LANDR directly  weve reached avery high level of sophistication over the last four years. Right now we do over 330,000 songs amonth, and if you think about it, thats more songs than all studios in America combined. Its really meeting the needs of alarge community of artists.
CloudBounce also offer an AI mastering service, and recently published awhite paper forecasting their future vision for AI-supported mastering, audio restoration and music production. Anssi Uimonen from CloudBounce points to the staggering volume of material uploaded to social media and video-sharing web sites: 300 hours of video are uploaded to YouTube every minute, and social media platforms such as Facebook are also hosting vast amounts of video content, typically with unprocessed and suboptimal audio.Anssi Uimonen of CloudBounce.The potential market for audio restoration and optimisation is enormous and growing. Anssi says that his aim is to build arobust, open network for AI audio production, enabling all content creators to make their videos, music and speech content sound the best it possibly can, automatically. CloudBounce illustrate this idea by asking us to think of acontent creator who records avideo using her iPhone; the content is great, but the level is too low and theres room reverberation. Although she doesnt have the skills or budget to pay for aprofessional, she uses AI tools to quickly optimise the audio quality so the content can be published online.
It may become second nature for content creators to drag and drop audio files onto AI-powered software, or use online services to quickly remove background noise, unwanted room reverberation and audio artifacts without any user involvement. With this volume of content there simply wouldnt be enough human audio engineers to go round.

AI is thus already opening up new markets by automatically mastering, restoring and optimising audio that would otherwise not have been processed. However, the CloudBounce white paper goes significantly further, and the company are seeking to open up their current cloud-based AI processing engine into anew community-driven ecosystem known as dBounce.
The teams long-term vision is to provide an online platform for AI-supported music production services. In time, the white paper envisages that users will be able to call upon AI services within dBounce to help with pretty much any music production task, including composition, mixing and mastering. As auser, you will work with dBounce as your AI audio producer, applying multiple algorithms to perform specific audio production tasks. These algorithms will progressively learn to understand different musical genres, and thus to make choices that will be sympathetic to your style. The automated EQ, compression and reverb settings that dBounce applies to your drums could thus sound very different depending on your musical genre.
A seasoned music producer could become part of this community ecosystem by teaching the dBounce algorithms specific audio production skills, such as how to EQ drums for aspecific genre, for example Scandinavian folk. Other users could then rate the quality of this EQ algorithm, so the dBounce community contributes real-world feedback on the sources from which dBounce is learning, as part of the trusted ecosystem. Community members who create the dBounce AI audio producers will be rewarded through tokens when their algorithms are used, which can be traded for other dBounce AI audio production services.
Users can also build dBounce algorithms by uploading audio libraries for feature detection to train the machine learning models, and evaluate or rank the quality of the resulting algorithms to ensure high quality. This call to arms will reward users with their own trusted profile within the community and tokens to purchase other AI services. The dBounce community ecosystem could also provide aplatform for plug-in manufacturers to draw upon AI audio producers to quickly perform production tasks such as mixing, so users could focus on other creative areas.
Dr Michael Terrell points to mastering as the low-hanging fruit of AI audio production, which requires little human interaction. The future road map for AI audio services is likely to become increasingly sophisticated and tailored to many different areas. Anssi adds that Within five to seven years we are starting to see some really scary things that are very advanced in terms of production and getting the initial idea... or even getting your initial ideas on composition, based on behaviour and stuff thats been learnt by the machine.
The dBounce white paper also offers other examples of the ways in which functional aspects of audio production, such as removing unwanted noise or ambience, could become automated. For example, content owners such as broadcasting companies may need to update the audio quality of their back catalogue to modern-day broadcasting standards. AI could quickly optimise this audio material, without the broadcasters needing to invest thousands of hours in repetitive manual tasks.
It seems reasonable to say many of these changes wont happen overnight, although theres plenty of ongoing research and development that is steadily introducing AI-supported music tools into our day-to-day lives.
LANDRs Pascal Pilon.Without exception, all the contributors Iinterviewed for this feature are passionate about AI as atool for enhancing, rather than replacing, human creativity. Many of them highlight the ways in which AI can empower people to be creative by automating the more functional aspects of music production, and by side-stepping the need to acquire difficult technical knowledge and skills. LANDRs Pascal Pilon says: Ithink its going to become aproducers world. Anybody with avision of asoundscape or specific song will be able to design it and make it areality, without having to learn instruments or needing any dexterity in the process. Its going to become more about visionaries of music, and Ithink it can only bring more quality out there.
Tamer Rashad likewise sees Humtap as helping music fans to become active creators, rather than passive listeners, saying that presently less than 1 percent of the population can currently contribute to music creation.Drew Silverstein is the founder of Amper Music. Drew Silverstein from Amper Music, meanwhile, describes their online service as an artificial intelligence composer, performer and producer that creates unique professional music tailored to any content in amatter of seconds. Ampers goal is to enable anyone around the world to be able to express themselves creatively through music  regardless of their background, access to resources or training.
Drew explains: Everyone has creative ideas. The challenge is not coming up with the idea, its figuring out how to express that idea. So if Ihave an idea for apainting but Ive never been trained, as soon as Itry to brush the canvas it might look terrible. Its not that my idea was bad, its that my ability to express my idea was insufficient.
If you are anon-musician we are now giving you the superpower, in essence: to be able to turn your idea into reality and to be able to make the content or the music that youve always envisioned, but havent always had the ability to do.

Dror Ayalon, the creator of loop-sharing environment Soundscape, sees AI as augmenting rather than replacing human capabilities. The AI and machine learning revolution is trying to solve things that we currently dont do  because we dont like to do them, or its too costly, or because we simply cant. Ithink its pointless to try and solve things that we already do. The future is basically the combination between human work and the machine. Its not about replacing the human. Most of the music we love, most of the sounds we love, most of the instruments we love will generally still be in the game. Its pointless if we dont do it.
As well as revolutionising existing music-production processes, AI is likely to create new markets for audio production services, as yet unknown. Perhaps surprisingly, alarge emerging market is for music fans, rather than solely musicians, producers and online content creators.
Many contributors point to amore connected future, where music consumers use AI music-creation apps to enrich their interactions with others on social media. Non-musicians could create music and collaborate with artists and friends, without any musical training, access to skilled producers, studios or money. This opens up opportunities for anyone with apassion for music, regardless of technical skills, to have acreative musical experience. Fans could propose new tracks directly to their artists, generated through their AI-supported interpretation. Tamer paints apicture of ateenage girl from avillage in Africa without access to studio facilities collaborating with her favourite artist using only her smartphone. AI could widen opportunities and level the playing field for everyone.
Empowering everyone to create music and interact through social media is amajor growth opportunity. AI turns passive listeners into music creators, bringing consumers much closer to artists. Several contributors draw aparallel with existing content-sharing platforms such as Instagram as the future model for connecting by creating new music, and social media is seen as offering huge potential for AI-produced music.
More daringly, Tamer Rashad suggests that AI could be used to create new tracks by artists who are no longer with us. Algorithms could analyse an artists entire back catalogue and identify patterns in the melodies, harmonies, instrumentation and arrangements, then generate limitless new songs inspired by that artist. Reproduction would be instrumental at least for the foreseeable future, although AI-created vocalists are seen as feasible in the longer term. Once again, all you would need is asmartphone.
The benefits of automating repetitive, non-creative functional activities are not only relevant to non-musicians but also to seasoned producers. As Dr Michael Terrell says, When you start anew project theres tons and tons of stuff you do thats similar every time but not exactly the same. Theres all these functional aspects that, if you have AI tools to assist, could probably shave off 50 to 70 percent of the time it actually takes to mix atrack. For those people being paid by the track, its alot to save.
Songwriters could likewise benefit from genre-appropriate, AI-suggested instrumentation or arrangements to help get asong over the line quickly  handy when youve an impending deadline. They could then use an app like Humtap to create afull demo in their desired genre and share it with the rest of the band. Musicians without the skills to mix asong could also rely upon AI-supported mixing; rather than having to learn how to use compressor controls such as ratio, threshold and release, you could simply say you want the vocals to be louder and more consistent, or that the guitar needs to be more prominent than the piano.
Tamer Rashad also suggests that AI could help artists redevelop their music in alternative genres, to reach anew audience. For example, an American pop artist could draw upon AI to regenerate an existing track in aBollywood style.
AI also has an important role to play in fostering collaboration between human musicians, and this is the core aim of Dror Ayalons upcoming online service Soundscape. Soundscape automatically matches your musical ideas with those other musicians: you record aloop, which Soundscape analyses and synchronises to acomparable musical recording from others around the world, evaluating how closely your music style matches theirs. All you need is asingle click of your mouse button and Soundscape makes the connections.
Dror Ayalon, inventor of Soundscape.You can then mould shared loops into your own music, just as others can with your musical phrases. Dror explains his motivation for Soundscape: Iwas always in search of new ways to get people closer to music-making and closer together. Ifelt that the ability to sync peoples loops automatically is just giving me an echo that there is someone out there playing something that is as weird as maybe what Iplay. Maybe its harmonics to what Iplay, maybe there are other people who are doing the same thing.
Dror summarises his experiences of connecting with others as this fascinating moment of not feeling alone  you, your guitar, your amp at home  was something that Ifelt Im really interested in. Since this is the case, your results are always surprising.
AI may also change the tools with which music producers do their work. As Dr Michael Terrell points out, digital audio workstation software and plug-ins are often modelled upon analogue ways of working, with amixing console and long-established types of processing such as compression and EQ. AI raises questions around our current way of interacting with DAWs. Will the analogue metaphor make way for radically different interfaces and interaction styles? Dr Terrell points to his research, which involves automating mundane tasks such as setting level balance and basic panning and EQ in order to allow musicians to focus on being musicians. He adds: My main motivation was to build atool that would help people to mix music well, having seen lots of friends who really struggled to do it. One of the issues around mixing is that its just quite complicated. You need agood technical understanding of what the audio processors are doing, you need reasonably good versions of those tools, you need decent ears, you need agood setup at home, and its just very, very difficult to do.
Dr Michael Terrell is one of the researchers behind dBounce.Dr Terrell suggests that AI tools could, in effect, act as an interface between the user and the standard DAW toolset. An AI interface could offer simple controls that would help you mix something that actually sounded better than it would have been if you tried to do it yourself, in alot less time. With all these tools you can build ahierarchy of controls where, right at one end, you have full automation, and as you step up, youre effectively providing more intricate controls, but through far easier-to-understand interfaces. So, when youre using acompressor, you have all these weird controls  you know attack, release, threshold, ratio. When you try to explain to someone what thats doing, its not simple. AI could enable musicians to say what they want as an outcome, such as I want to sound abit louder or a bit more consistent. That way, youre allowing people to control the software using an interface that they can easily understand.
Drew Silverstein from Amper Music continues the theme of AI offering simpler tools. Those [DAWs] are complicated. They take time and skill to learn, which speaks of the immense power that they hold  theyre complex for areason. For individuals who are used to using those pieces of advanced technology, Ampers not going to replace them. Amper is going to be acollaborative tool and acreative partner to help [people] further their existing music-making in the same way that the DAW did for them 20 years ago. Amper will do the same thing. Youve got awhole population of individuals who look at aDAW and say its too much. Were not saying Lets get rid of that knowledge and expertise as aprerequisite to be able to express your creative idea. If you use aDAW, great. Youll probably be able to get more out of Amper than anyone else would, or youll be able to go further. But youre not required to.
It seems reasonable to summarise that as algorithms become more widely available, AI may enable us to define the level of control we want over our production. We can still have lots of complexity and flexibility if we want it, but we can also choose easier and more immediate results, albeit with more decisions made for us. AI could become like ahuman engineer who quietly sets things up and helpfully manages the technology for you, so you can focus on the music.
As Dror Ayalon says, If you think about it, for many years, the industry emulated analogue instruments in adigital environment. The basic reason behind that was to reduce cost on one hand and to be able to do things on amassive scale. Ithink that people are starting to understand now that were not actually composing music, were composing data. We basically produce data pieces, but this data is artistic data that we can actually change minds and lives with  so Ithink you will see software starting to build tools and interfaces that will allow musicians to actually interact with that. Ibelieve that you will be able to find tools that are heavily rooted in artificial intelligence in any studio and on the phone of any artist. Ibelieve this age is coming real soon.
If AI can, hypothetically, produce an album in seconds, does this mean that musicians and producers will be out of ajob, and that there is no longer arole for human creativity? Thats not the way our interviewees see it. Without exception, everyone we contacted loves human involvement in music creation. As Pascal Pilon says, For us at LANDR, its essential that when we talk about the AI, we always carve the creative aspects out of the equation and leave them in the hands of the artist. We see ourselves empowering artists to make creative decisions and we focus on taking the technical elements out of the equation.
Dror Ayalon, the creator of Soundscape, describes AI as atool to improve musical creativity: If these algorithms create something that is not interesting  that sounds exactly like the Beatles, or exactly like hip-hop hits from last year  they wont succeed and get enough attention. So its about giving creative tools to people to make innovative music on one hand and, on the other hand, using algorithms to generate synthesized new music in the most creative way possible. No-one in the field that Iknow about is in it just to replicate something that was done before. Every one of us is excited about the vision, excited about creativity, and if artists and sound engineers wont adopt what weve built, we failed. So [were] working with these people to understand how can we help them do what they do. Were not looking to replace anyone, but build tools that can make them do amazing things. Most of the innovation that we saw in music production in the past came from innovative tools, from innovative synthesizers to innovative acoustic instruments to innovative computer music. Ithink there is abig opportunity here for something that is dramatically different that could change the convention about how we make, how we mix, how we produce music. Ibelieve that Ican say the music production and music composition industries are looking for such innovation.
Drew from Amper Music adds: Regarding the concern around diminishing creativity, Ithink that is an argument that some would make without looking at the whole picture, because, arguably, the fact that people now can produce something or make something without knowing how to perform and write for every instrument and perform with live players already kind of does that. Weve just accepted the current standard as OK, but its the fact that akid in his bedroom can now make abeat or ahip-hop loop or asong without knowing so many things that, 100 years ago, we would have said If you dont know this, you are not creative. Evolution is an unstoppable part of humanity in any field.
Iwould actually argue that even today the bar to create music and produce music is quite low, and doesnt necessarily require any skill. At one point hundreds of years ago, to write and create music you needed to know music theory and to have apen and paper. You had to be achurch musician or have asponsor. Now, if you have acellphone and some rudimentary app you can make music that could be ahit, just because youve got technology that requires very little skill or expertise.
That being said, there is still aworld in which the more skill you have and the more training you have, arguably, the stronger your ability to express your idea creatively would be. As technology advances, the technological bar necessary to express yourself drops considerably. We went from pen and paper to magnetic tape to digital to mobile  every time we do that it gets easier and easier to take your idea and turn it into reality. As aconsequence, the skill set and the ability to actually do that becomes more and more democratised, and so more and more are able to join the creative class whereas they previously werent able to.
Dr Michael Terrell makes the point that One of the big reasons people make music is that they like making it. Its not some task that you want to farm out to AI, its something that people actually enjoy. So what you end up doing when someone wants to do something creative like that is you build tools that help them do that more effectively. So Ithink this idea that it reduces creativity is actually wrong. Its the opposite. It should free up people to actually be able to make the music they want to make, without these technical burdens that are abig limitation for alot of people.
So how will this affect people who make their living from music production? Pascal Pilon: When we launched this thing [LANDR] our slogan was that we work for musicians. Our view was never to prevent engineers from making aliving. What we looked at was that over 95 percent of music creators could not afford the service of mastering engineers. Even those that could afford mastering engineers, would only do so now and then, for example for one album ayear instead of every creation. What we believe is that every time someone creates audio, it deserves to be delivered and finished with afighting chance, so people can look at it and say Hey, there is something in there that Ithink Ican work on and improve. Its just ready to be shared with the people that Ilike.
Ithink theres alot mastering engineers out there who are very, very good, and who are also acting as producers in their approach. They revisit and direct the overall sound of the mix  and that, to me, is producing. And thats great. Im really happy master engineers act as co-producers, in that were just bringing productivity to artists, were not taking jobs. So Idont think theres going to be less sound engineering jobs with LANDR around. Ithink theres just going to be more music. Ithink thats great.
Dr Michael Terrell talks about future opportunities for professional producers to train dBounce to become an AI audio producer. One of the things weve tried to do with the design of this ecosystem is that if youre amusic producer and youre very good, you will still be  you can actually take part in this ecosystem. If you can train AI and you own it, you can earn money from that AI, so it becomes an ancillary revenue stream. If you can dump all your AI knowledge into amachine and then earn the money from that, you can probably enjoy your life doing whatever you want to do. So its about getting involved with the ecosystem.
Drew Silverstein from Amper Music again draws an analogy with earlier technologies. When the synthesizer came out, everyone worried that there would never be another live musician ever again. Orchestras are still around, and some are thriving more than ever. Depending on what area you put yourself in, theres always going to be this existential concern about the future. What weve seen is that for those who take advantage of the opportunities provided to them by advancing technology, the future in some ways is never brighter.
Drew draws adistinction between artistic and functional music. We look at media music in two different buckets. One we call artistic music and one we call functional music. Artistic music is valued for the collaboration and creativity that goes into making it. Its about the creative process of working with another person to make art thats artistic. Our perspective is that that will never go away, even when Amper and AI music is indistinguishable from human-created music. This is because the value is not about the final product, its about the process of making it. Its about [John] Williams and [Steven] Spielberg sitting down and scoring the next Star Wars. Its that artistic mind-melding of multiple individuals making art.
Functional music, by contrast, is music that exists in aworld where it serves apurpose  but no-one gets in an elevator and asks about the music, what its doing and why its there. Functional music is valued for the purpose that it meets, but not for the creative process that went into making it. In those situations individuals who [need] functional music often have creative ideas themselves  musical ideas. We aim to help them directly turn their idea into reality.
And so what Isay to people who are concerned about jobs is acouple of things. One, your job in five years will probably not exist. Your career certainly will, and its important those two things dont get separated out of context  because otherwise it can make me look terrible when Idont intend it to be! What Imean is the way that we make aliving in music has changed for all time as technology advances. As long as were able to adapt to the advances and to take advantage of what technology offers us, then well be able to continue providing value to others. Well be able to keep our career, but that doesnt require that our job remains the exact same, because its not and it shouldnt be. If it was then where would progress be? We would be stuck in whatever era you want to define as the era we should be stuck in. So part of the negative reaction to this evolution is fear based on an uncertainty within which an individual exists: Ive spend my entire life trying to do this thing. What happens to me in this future that as of now is very opaque, and because of that, is very concerning to me.
Drew adds that while Amper will change the future of music, it wont replace it. As long as an individual is creating music which is valued artistically that is delivering value to whoever it is  whether that is alistener or aclient or creator  in the way that they need value to be delivered, then humans will be able to create and make aliving from music or other art for all time.
Similarly, it seems plausible that the best producers will always be in demand. It looks likely that AI will not replicate all the interactions and deep working relationships between artist and producer any time soon. In essence, people like people.
If AI apps empower everybody to generate music, is there arisk that great music will simply become lost in huge amounts of online content? Pascal Pilon compares music with video content: If you think about the quality of lenses today and what people are using from their phones, theres aton of pictures being taken and theres alot of videos being made. Are we saying that there are too many videos? Not so much, we have the attention span that we do. YouTube is full of videos, and we watch what we want to watch. Ithink music is becoming something we access via recommendations from friends and people. Ithink the way we navigate the offering will change.
Pascal also points out that AI tools can help navigate the flood of music, helping us to find artists to listen to or work with. In terms of sounds that youre looking for, in terms of collaborators you want to work with, in terms of listeners you should be listening to, Ithink AI will make that process very efficient. Making music and getting heard will become more like posting on Instagram and getting followers. Everythings going to be residing more on social networks, and Ithink the music landscape will trend towards the image landscape  very fragmented, very wide. Youre still going to be able to buy professional photography at $20,000. That hyper quality will still be featured in acertain way, but listeners tastes will prevail. Idont think that it matters if theres five million songs or five billion songs out there. Imean most of those songs will be only important to the artists themselves and their immediate relatives. The best will stand out, and the methods by which we find them will be very exciting. Our relationship to music will be more social-media based, as will our response to music.
The future is likely to see an explosion of new music created by people who previously did not have the means to do so, enabled by AI-based tools. Equally, musicians and producers are likely to find more time to be creative and to create music in new ways, leaving AI to manage repetitive functional tasks. Creating music is likely to become part of peoples online interactions, with consumers creating and sharing new music through social media platforms.
AI for music production and consumption is ahuge area, and this article can only offer aflavour of ongoing research and development for AI music services. As Cliff Fluet puts it, AI is going to become aterm thats as helpful as talking about the Internet. Twenty years ago, if Id tried to describe the Internet, you would think it was just one platform  and actually it was the first step to something huge.
The rich opportunities around AI raise many questions, and to date, we only have some answers. Many developments are in their infancy and some timings remain uncertain. What is certain is that AI is receiving serious attention from major record labels, number one artists, business incubators and leading-edge software developers, so changes are on their way. Many contributors estimate well start to see ashift in music production and consumption within the next five to seven years. Widespread adoption will come down to how easily we can interact with our AI tools and the quality of the end results, which are steadily advancing.
As with any emerging technology, we clearly have achoice about whether to use it and how we want to use it, and it may be the market that shapes the direction of AI software. All of our contributors were keen to emphasise that AI will not replace artists or producers. Paradoxically, the more we use AI within music, the more we may draw people together.
If AI changes the music-industry landscape, how might industry bodies such as major record labels respond?Cliff Fluet of media advisory firm Eleven.Cliff Fluet, Managing Director from media technology advisory firm Eleven, says: Because there are so many different types of AI music tools, the response from major labels really depends on what the technology is and what it does. To shape-shift [change the genre of] tracks or make them adapt or remix them at avast speed is extremely interesting to them. These are tools that make what was avery manual, costly and laborious process fast, cheap and efficient. In aworld where everythings run by streaming web site playlists, they can potentially create aversion of asong for every genre  to have aversion in every playlist, thats money for them. For sync, labels make alot of money from using music in ads and for brands, so AI technology that adapts existing recordings more easily is also popular.There is much excitement, but acertain hesitation to ask incumbent labels to embrace pure generative AI. Theres Upton Sinclairs quote  It is difficult to get aman to understand something if his salary depends on not understanding it  and that very much is applicable to generative AI music. If you live in aworld where musicians and/or A&R people felt that amachine could just create aperformance... one might say theyre intellectually invested in not taking it up.Taking the long view, because Im really, really old... Ive seen all this before. There was word in the 70s that synthesizers meant there would be no more pianists or keyboards, or drum machines meant no more drummers. There was then aword that music sampling would destroy music, or that putting digital music online would mean the end. Not only did none of those things turn out to be true, but the exact opposite turned out to be true.LANDRs Pascal Pilon points out that The music industry is alaggard in many aspects. Look at the financial market and how start-ups around the world are seeking financing to develop and commercialise products with the assistance of venture capital partners. In the music industry, this role has been played historically by the record label, but sourcing adeal with labels is still very rudimentary. Ithink the music industrys financing model will progressively become more data-driven, more competitive. It will draw more capital from awider set of investors, still involving labels but also alarger set of small to large angel investors.Pascal predicts that music labels will still need to invest heavily in talent to promote music. Even if theres AI, you will still need people to bring artists visibility. Think marketing: the rarest thing you get is listeners in the music space that spend. Your ability to bring people to listen to you will still require acast of collaborators from amarketing perspective. Again, its very similar to the movie industry. Look at how important the production budget is  yes theyre huge, but think about the commercial budget, which is even bigger. To gain peoples attention, you will need to spend on channels to deliver that  think Facebook. If you can bring virality to the likes of Google and YouTube then you will be able to be independent, but it will require lots of sophistication from apromotion standpoint.
Who owns music that is generated by artificial intelligence? And if AI software can be inspired by existing songs and generate new music of a similar style, does copyright become agrey area?Like every new area of technology, AI promises to generate plenty of work for lawyers.Cliff Fluet of media advisory firm Eleven insists: Its not grey at all. Its that people dont like to hear the answer, depending on what theyre doing. But youve asked the key question and theres awhole bunch of lawyers speculating on this stuff!This is as good or as bad aquestion as asking who owns amovie. The real answer is that it kind of depends on how you made it, how much of it is yours and how much of the stuff is owned by somebody else. To what extent are you getting the input of actors, set designers, third parties and so on? To what extent did you create something wholly original? The answer is as simple and sophisticated as that really. Every single one is dependent on how their processes, how their algorithms, how their choices are made. If youre areally good lawyer the answer is very, very simple, provided you really understand the depths of the processes.Given that AI for music production is an emerging technology, is this alegally tested area?Cliff Fluet: Id say it is and it isnt. The law on music copyright has been built upon highly unreliable technology called musicians and the human brain. Youve got people asking What if amachine replicates something else?  and its like Well, what if an artist replicates something else?, which is what happens every single day in the music industry. Theres real questions about did someone actually hear something or did they not? Did someone actually adapt something? The law is not different when its amachine or human being. The difference between ahuman being and amachine is that with amachine you have absolute evidence.Cliff points to previous legal cases where the jury has to sit there and try and work out whether someone heard something, adapted something or did it. With amachine, you dont have to rely on its memory, you just literally can see all of the notes there, so actually Ithink arguably its easier rather than more difficult. Working out whether or not ahuman has ripped off acomposition or whether or not they own it, or whether or not they shared it, is far, far more complex than amachine. If anything, the advancements in technology could help.
The services and products mentioned in the main text can be accessed through the following URLs:

