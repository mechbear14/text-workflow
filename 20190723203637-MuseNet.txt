MuseNet

Samples
Since MuseNet knows many different styles, we can blend generations in novel ways. Here the model is given the first 6 notes of a Chopin Nocturne, but is asked to generate a piece in a pop style with piano, drums, bass, and guitar. The model manages to blend the two styles convincingly, with the full band joining in at around the 30 secondmark:
Try MuseNet
Were excited to see how musicians and non-musicians alike will use MuseNet to create new compositions!
In simple mode (shown by default), youll hear random uncurated samples that weve pre-generated. Choose a composer or style, an optional start of a famous piece, and start generating. This lets you explore the variety of musical styles the model can create. In advanced mode you can interact with the model directly. The completions will take longer, but youll be creating an entirely newpiece.

Some of MuseNets limitationsinclude:

The instruments you ask for are strong suggestions, not requirements. MuseNet generates each note by calculating the probabilities across all possible notes and instruments. The model shifts to make your instrument choices more likely, but theres always a chance it will choose somethingelse.
MuseNet has a more difficult time with odd pairings of styles and instruments (such as Chopin with bass and drums). Generations will be more natural if you pick instruments closest to the composer or bands usualstyle.

Composer and instrumentation tokens
We created composer and instrumentation tokens to give more control over the kinds of samples MuseNet generates. During training time, these composer and instrumentation tokens were prepended to each sample, so the model would learn to use this information in making note predictions. At generation time, we can then condition the model to create samples in a chosen style by starting with a prompt such as a Rachmaninoff pianostart:
Or prompted with the band Journey, with piano, bass, guitar, anddrums:
We can visualize the embeddings from MuseNet to gain insight into what the model has learned. Here we use t-SNE to create a 2-D map of the cosine similarity of various musical composer and styleembeddings.



  Hover over a specific composer or style to see how it relates toothers.


ChopinLisztBroadwayWagnerMozartFleetwood MacNine Inch NailsRobbie WilliamsThe Beach BoysThe BeatlesMariah CareyShania TwainMichael JacksonRicky MartinDvorakBeethovenHaydnEnyaDebussyElvis PresleySpice GirlsMadonnaJazzGershwinClementiFaureBon JoviBachBluesLady GagaAfricanAdeleKaty PerryBollywoodTchaikovskyRachmaninoffRavelSchubertAlbenizGranadosMendelssohnSatieScarlattiFranckPachelbelByrdWhitney HoustonJourneyBrahmsGreen DayBob MarleyBritney SpearsQueenAretha FranklinSchumannJoplinDisney

Long-term structure
MuseNet uses the recompute and optimized kernels of Sparse Transformer to train a 72-layer network with 24 attention headswith full attention over a context of 4096 tokens. This long context may be one reason why it is able to remember long-term structure in a piece, like in the following sample imitatingChopin:
It can also create musical melodic structures, as in this sample imitatingMozart:
Music generation is a useful domain for testing the Sparse Transformer as it sits on a middle ground between text and images. It has the fluid token structure of text (in images you can look back N tokens and find the row above, whereas in music theres not a fixed number for looking back to the previous measure). Yet we can easily hear whether the model is capturing long term structure on the order of hundreds to thousands of tokens. Its much more obvious if a music model messes up structure by changing the rhythm, in a way that its less clear if a text model goes on a brieftangent.
Dataset
We collected training data for MuseNet from many different sources. ClassicalArchives and BitMidi donated their large collections of MIDI files for this project, and we also found several collections online, including jazz, pop, African, Indian, and Arabic styles. Additionally, we used the MAESTROdataset.
The transformer is trained on sequential data: given a set of notes, we ask it to predict the upcoming note. We experimented with several different ways to encode the MIDI files into tokens suitable for this task. First, a chordwise approach that considered every combination of notes sounding at one time as an individual chord, and assigned a token to each chord. Second, we tried condensing the musical patterns by only focusing on the starts of notes, and tried further compressing that using a byte pair encodingscheme.
We also tried two different methods of marking the passage of time: either tokens that were scaled according to the pieces tempo (so that the tokens represented a musical beat or fraction of a beat), or tokens that marked absolute time in seconds. We landed on an encoding that combines expressivity with conciseness: combining the pitch, volume, and instrument information into a singletoken.
Sample encoding which combines pitch, volume, and instrument.
During training,we:

Transpose the notes by raising and lowering the pitches (later in training, we reduce the amount of transposition so that generations stay within the individual instrumentranges).
Augment the volumes, turning up or turning down the overall volumes of the varioussamples.
Augment timing (when using the absolute time in seconds encoding), effectively slightly slowing or speeding up thepieces.
Use mixup on the token embeddingspace

We also create an inner critic: the model is asked during training time to predict whether a given sample is truly from the dataset or if it is one of the models own past generations. This score is used to select samples at generationtime.
Embeddings
We added several different kinds of embeddings to give the model more structural context. In addition to the standard positional embeddings, we added a learned embedding that tracks the passage of time in a given sample. This way, all of the notes that sound at the same time are given the same timing embedding. We then add an embedding for each note in a chord (this mimics relative attention, since it will be easier for the model to learn that note 4 needs to look back at note 3, or else at note 4 of the previous chord). Finally, we add two structural embeddings which tell the model where a given musical sample is within the larger musical piece. One embedding divides the larger piece into 128 parts, while the second encoding is a countdown from 127 to 0 as the model approaches the (end)token.

Were excited to hear what people create! If you create a piece you like, you can upload it to a free service like Instaudio and then tweet us the link (the MuseNet demo has a tweet button to help withthis).
If youre interested in learning more about OpenAIs music work, consider applying to join our team. Please feel free to email us with suggestions for the MuseNet demo. Wed also love to hear from you if youre interested in composing with MuseNet in more depth, or if you have MIDI files youd like to add to the trainingset.
MuseNet played an experimental concert on April 25th, 2019, livestreamed on OpenAIs Twitch channel, in which no human (including us) had heard the pieces before.

